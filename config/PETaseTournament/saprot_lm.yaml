setting:
  seed: 20000812
  os_environ:
    WANDB_API_KEY: ~
    WANDB_RUN_ID: ~
    CUDA_VISIBLE_DEVICES: ""
    MASTER_ADDR: localhost
    MASTER_PORT: 12315
    WORLD_SIZE: 1
    NODE_RANK: 0
  wandb_config:
    project: Thermostability_LM
    name: SaProt_35M_AF2_LM

model:
  # 1. Change the model wrapper to the LM model
  model_py_path: saprot/saprot_lm_model
  kwargs:
    config_path: weights/PLMs/SaProt_35M_AF2
    load_pretrained: True

  lr_scheduler_kwargs:
    last_epoch: -1
    init_lr: 2.0e-5
    on_use: false

  optimizer_kwargs:
    betas: [0.9, 0.98]
    weight_decay: 0.01

  save_path: weights/Thermostability/SaProt_35M_AF2_custom_LM20.pt

dataset:
  # 2. Change the dataset wrapper to the LM dataset
  dataset_py_path: saprot/saprot_lm_dataset
  dataloader_kwargs:
    batch_size: 8
    num_workers: 0 # Keep 0 for Mac/CPU

  train_lmdb: LMDB/custom_lm/train
  valid_lmdb: LMDB/custom_lm/valid
  test_lmdb: LMDB/custom_lm/test
  
  # 3. Update kwargs for LM specific arguments
  kwargs:
    tokenizer: weights/PLMs/SaProt_35M_AF2
    max_length: 512
    mask_ratio: 0.15       # Probability of masking a token
    use_bias_feature: False # Set to True only if your LMDB has 'coords' data

Trainer:
  max_epochs: 20
  log_every_n_steps: 5
  strategy:
    find_unused_parameters: True
  logger: False
  enable_checkpointing: false
  val_check_interval: 0.5
  accelerator: cpu
  devices: 1
  num_nodes: 1
  accumulate_grad_batches: 2
  precision: 32
  num_sanity_val_steps: 0
